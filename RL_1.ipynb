{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL Simple Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the NChain-v0 state machine of gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Some Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source code for this environment\n",
    "- https://github.com/openai/gym/blob/master/gym/envs/toy_text/nchain.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Details about the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This env presents moves along a linear chain of states.\n",
    "\n",
    "## *Two actions*: Forward (0) and Backward(1)\n",
    "### 1. **Forward**, which moves along the chain but returns no reward\n",
    "### 2. **Backward**, which returns to the beginning and has a small reward\n",
    "\n",
    "## *Total States*: *5 States* (0 to 4)\n",
    "\n",
    "## Reward\n",
    "### 1. Backward action gives reward of 1.\n",
    "### 2. Forward gives no reward until the final stage. Reward of staying in state 4 is 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the environment\n",
    "\n",
    "from nchain import *\n",
    "env = NChainEnv(slip=0, small=1, large=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reset is used to initialize the env. Often it sets the environment to the begining state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting State is:  0\n"
     ]
    }
   ],
   "source": [
    "Starting_State = env.reset()\n",
    "print(\"Starting State is: \",Starting_State)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal: Move throught the chain. Print states and the reward\n",
    "\n",
    "## env.step(action), returns a tuple\n",
    "- Tuple is: (new state, reward, done, info)\n",
    "- Environment gives done after 1000 steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute 10 forward actions\n",
    "- After reaching the state 5, agent stays there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  1 cur_state: 0  Action: 0  next_state: 1  Reward:  0  Done:  False\n",
      "Step:  2 cur_state: 1  Action: 0  next_state: 2  Reward:  0  Done:  False\n",
      "Step:  3 cur_state: 2  Action: 0  next_state: 3  Reward:  0  Done:  False\n",
      "Step:  4 cur_state: 3  Action: 0  next_state: 4  Reward:  0  Done:  False\n",
      "Step:  5 cur_state: 4  Action: 0  next_state: 4  Reward:  100  Done:  False\n",
      "Step:  6 cur_state: 4  Action: 0  next_state: 4  Reward:  100  Done:  False\n",
      "Step:  7 cur_state: 4  Action: 0  next_state: 4  Reward:  100  Done:  False\n",
      "Step:  8 cur_state: 4  Action: 0  next_state: 4  Reward:  100  Done:  False\n",
      "Step:  9 cur_state: 4  Action: 0  next_state: 4  Reward:  100  Done:  False\n",
      "Step:  10 cur_state: 4  Action: 0  next_state: 4  Reward:  100  Done:  False\n"
     ]
    }
   ],
   "source": [
    "Done = False\n",
    "step = 0\n",
    "\n",
    "cur_state = env.reset()\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    #execute forward action\n",
    "    action = 0\n",
    "    \n",
    "    next_state,r,Done,_ = env.step(action)\n",
    "    \n",
    "    step = step + 1\n",
    "    print(\"Step: \",step, \"cur_state:\",cur_state, \" Action:\",action,\" next_state:\",next_state,\" Reward: \",r,\" Done: \",Done)\n",
    "    cur_state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute 10 backward actions\n",
    "- Agent stays in state 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  1 cur_state: 0  Action: 1  next_state: 0  Reward:  1  Done:  False\n",
      "Step:  2 cur_state: 0  Action: 1  next_state: 0  Reward:  1  Done:  False\n",
      "Step:  3 cur_state: 0  Action: 1  next_state: 0  Reward:  1  Done:  False\n",
      "Step:  4 cur_state: 0  Action: 1  next_state: 0  Reward:  1  Done:  False\n",
      "Step:  5 cur_state: 0  Action: 1  next_state: 0  Reward:  1  Done:  False\n",
      "Step:  6 cur_state: 0  Action: 1  next_state: 0  Reward:  1  Done:  False\n",
      "Step:  7 cur_state: 0  Action: 1  next_state: 0  Reward:  1  Done:  False\n",
      "Step:  8 cur_state: 0  Action: 1  next_state: 0  Reward:  1  Done:  False\n",
      "Step:  9 cur_state: 0  Action: 1  next_state: 0  Reward:  1  Done:  False\n",
      "Step:  10 cur_state: 0  Action: 1  next_state: 0  Reward:  1  Done:  False\n"
     ]
    }
   ],
   "source": [
    "Done = False\n",
    "step = 0\n",
    "\n",
    "cur_state = env.reset()\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    #execute backward action\n",
    "    action = 1\n",
    "    \n",
    "    next_state,r,Done,_ = env.step(action)\n",
    "    \n",
    "    step = step + 1\n",
    "    print(\"Step: \",step, \"cur_state:\",cur_state, \" Action:\",action,\" next_state:\",next_state,\" Reward: \",r,\" Done: \",Done)\n",
    "    cur_state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Execute 10 random actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step:  1 cur_state: 0  Action: 1  next_state: 0  Reward:  1  Done:  False\n",
      "Step:  2 cur_state: 0  Action: 0  next_state: 1  Reward:  0  Done:  False\n",
      "Step:  3 cur_state: 1  Action: 0  next_state: 2  Reward:  0  Done:  False\n",
      "Step:  4 cur_state: 2  Action: 0  next_state: 3  Reward:  0  Done:  False\n",
      "Step:  5 cur_state: 3  Action: 0  next_state: 4  Reward:  0  Done:  False\n",
      "Step:  6 cur_state: 4  Action: 1  next_state: 0  Reward:  1  Done:  False\n",
      "Step:  7 cur_state: 0  Action: 0  next_state: 1  Reward:  0  Done:  False\n",
      "Step:  8 cur_state: 1  Action: 0  next_state: 2  Reward:  0  Done:  False\n",
      "Step:  9 cur_state: 2  Action: 1  next_state: 0  Reward:  1  Done:  False\n",
      "Step:  10 cur_state: 0  Action: 1  next_state: 0  Reward:  1  Done:  False\n"
     ]
    }
   ],
   "source": [
    "Done = False\n",
    "step = 0\n",
    "\n",
    "cur_state = env.reset()\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    #execute random actions\n",
    "    action = random.randint(0,1)\n",
    "    \n",
    "    next_state,r,Done,_ = env.step(action)\n",
    "    \n",
    "    step = step + 1\n",
    "    print(\"Step: \",step, \"cur_state:\",cur_state, \" Action:\",action,\" next_state:\",next_state,\" Reward: \",r,\" Done: \",Done)\n",
    "    cur_state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value function evaluation\n",
    "## 1. Fix the policy\n",
    "## 2. Execute the policy for some steps and find the value of each state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value function for state: 0 using forward policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function is: 99600.0\n",
      "Discount is: 1.0\n"
     ]
    }
   ],
   "source": [
    "env.set_state(0)\n",
    "\n",
    "num_steps = 1000\n",
    "\n",
    "reward = 0\n",
    "discount = 1.0\n",
    "factor = 1.0\n",
    "\n",
    "for i in range(num_steps):\n",
    "    #execute forward action: 0\n",
    "    action = 0\n",
    "    next_state,r,Done,_ = env.step(action)\n",
    "    reward = reward + discount*r\n",
    "    discount = discount * factor\n",
    "    \n",
    "print(\"Value Function is:\",reward)\n",
    "print(\"Discount is:\",discount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value function for state: 0 using random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function is: 3675.112\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 500\n",
    "Total_reward = 0\n",
    "start_state = 0\n",
    "\n",
    "for j in range(num_episodes):\n",
    "\n",
    "    env.set_state(start_state)\n",
    "    num_steps = 1000\n",
    "\n",
    "    reward = 0\n",
    "    discount = 1.0\n",
    "    factor = 1.0\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        #execute random actions\n",
    "        action = random.randint(0,1)\n",
    "\n",
    "        next_state,r,Done,_ = env.step(action)\n",
    "        reward = reward + discount*r\n",
    "        discount = discount * factor\n",
    "    \n",
    "    Total_reward = Total_reward+reward\n",
    "    \n",
    "print(\"Value Function is:\",Total_reward/num_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value function for state: 0 using backward policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function is: 1000.0\n",
      "Discount is: 1.0\n"
     ]
    }
   ],
   "source": [
    "env.set_state(start_state)\n",
    "\n",
    "num_steps = 1000\n",
    "\n",
    "reward = 0\n",
    "discount = 1.0\n",
    "factor = 1.0\n",
    "\n",
    "for i in range(num_steps):\n",
    "    #execute backward action: 1\n",
    "    action = 1\n",
    "    \n",
    "    next_state,r,Done,_ = env.step(action)\n",
    "    reward = reward + discount*r\n",
    "    discount = discount * factor\n",
    "    \n",
    "print(\"Value Function is:\",reward)\n",
    "print(\"Discount is:\",discount)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Value function for state: 4 using random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function is: 3728.182\n"
     ]
    }
   ],
   "source": [
    "num_episodes = 500\n",
    "Total_reward = 0\n",
    "start_state = 4\n",
    "\n",
    "env.reset()\n",
    "\n",
    "for j in range(num_episodes):\n",
    "\n",
    "    env.set_state(start_state)\n",
    "    num_steps = 1000\n",
    "\n",
    "    reward = 0\n",
    "    discount = 1.0\n",
    "    factor = 1.0\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        #execute random actions\n",
    "        action = random.randint(0,1)\n",
    "\n",
    "        next_state,r,Done,_ = env.step(action)\n",
    "        reward = reward + discount*r\n",
    "        discount = discount * factor\n",
    "    \n",
    "    Total_reward = Total_reward+reward\n",
    "    \n",
    "print(\"Value Function is:\",Total_reward/num_episodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discuss the observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative value function calculation: Random Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "action_0 = 0.5\n",
    "action_1 = 1-action_0\n",
    "\n",
    "discount = 1.0\n",
    "\n",
    "def Calculate_Value_Function(env, itr=1000):\n",
    "    \n",
    "    #initialize the value function\n",
    "    v_table = np.zeros((5))\n",
    "    \n",
    "    for i in range(itr):\n",
    "        \n",
    "        #5 states\n",
    "        for j in range(5):\n",
    "            \n",
    "            env.set_state(j)\n",
    "            #next state with action 0\n",
    "            next_state_0,r_0,_,_ = env.step(0)\n",
    "            \n",
    "            \n",
    "            env.set_state(j)\n",
    "            #next state with action 1\n",
    "            next_state_1,r_1,_,_ = env.step(1)\n",
    "            \n",
    "            v_table[j] = (action_0*r_0+action_1*r_1) + discount*(action_0*v_table[next_state_0]+ action_1*(v_table[next_state_1]))\n",
    "        #print(i,v_table)\n",
    "    return v_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4815.05555556 4823.72222222 4836.22222222 4861.22222222 4911.22222222]\n"
     ]
    }
   ],
   "source": [
    "v_table = Calculate_Value_Function(env,itr=1000)\n",
    "print(v_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Poliocy\n",
    "## [4815.05555556 4823.72222222 4836.22222222 4861.22222222 4911.22222222]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Improvement follow: S0 ---> S1----> S2 ----> S3 ----> S4(Self-Loop) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps:\n",
    "## 1. Start with initial Q values.\n",
    "## 2. Find the action to take using the current Q values.\n",
    "## 3. Take the action, and update the Q values.\n",
    "## 4. Repeat step 2 and step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Learning with Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_with_table(env, num_episodes=500):\n",
    "    \n",
    "    # Stores the Q values: 5 states and 2 actions\n",
    "    q_table = np.zeros((5, 2))\n",
    "    \n",
    "    \n",
    "    gamma = 0.99  # Discount\n",
    "    alpha = 0.8  # Learning rate\n",
    "    \n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        \n",
    "        #print(\"episode is: \",i)\n",
    "        \n",
    "        steps = 1000\n",
    "        for i in range(steps):\n",
    "            \n",
    "            if np.sum(q_table[s,:]) == 0: \n",
    "                \n",
    "                # we don't which is the better value\n",
    "                # Make a random selection of actions\n",
    "                a = np.random.randint(0, 2)\n",
    "            \n",
    "            else:\n",
    "                # select the action with largest q value in state s\n",
    "                a = np.argmax(q_table[s, :])\n",
    "            \n",
    "            # Take the action\n",
    "            new_s, r, done, _ = env.step(a)\n",
    "            \n",
    "            # update the Q value\n",
    "            q_table[s, a] += r + alpha*(gamma*np.max(q_table[new_s, :]) - q_table[s, a])\n",
    "            s = new_s\n",
    "            \n",
    "            #print(\"Steps: \",i, a, new_s, r, done)\n",
    "           \n",
    "            \n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., 125.],\n",
       "       [  0.,   0.],\n",
       "       [  0.,   0.],\n",
       "       [  0.,   0.],\n",
       "       [  0.,   0.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_learning_with_table(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Something is wrong\n",
    "### There isnâ€™t enough exploration in the training method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We are adding exploring now\n",
    "### Adaptively explore the state space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explorative_q_learning_with_table(env, num_episodes=500):\n",
    "    \n",
    "    q_table = np.zeros((5, 2))\n",
    "    \n",
    "    gamma = 0.99  # Discount\n",
    "    \n",
    "    eps = 0.99 # Exploration factor\n",
    "    \n",
    "    alpha = 0.8  # Learning rate\n",
    "    \n",
    "    eps_factor = 0.999 # Exploration decay\n",
    "    \n",
    "    for i in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        \n",
    "        eps = eps*eps_factor\n",
    "        \n",
    "        steps = 1000\n",
    "        \n",
    "        for i in range(steps):\n",
    "            \n",
    "            # select the action with highest cummulative reward\n",
    "            if np.random.random() < eps or np.sum(q_table[s, :]) == 0:\n",
    "                a = np.random.randint(0, 2)\n",
    "            else:\n",
    "                a = np.argmax(q_table[s, :])\n",
    "           \n",
    "            new_s, r, done, _ = env.step(a)\n",
    "            q_table[s, a] += r + alpha * (gamma * np.max(q_table[new_s, :]) - q_table[s, a])\n",
    "            s = new_s\n",
    "        \n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[12007.450125  , 11888.62562375],\n",
       "       [12128.7375    , 11888.62562375],\n",
       "       [12251.25      , 11888.62562375],\n",
       "       [12375.        , 11888.62562375],\n",
       "       [12500.        , 11888.62562375]])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explorative_q_learning_with_table(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation: At every state the reward is more for the action '0': Forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
